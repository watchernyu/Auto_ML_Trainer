{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ML final project\n",
    "#Yu Zhou, Che Wang\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.image as mpimg\n",
    "from glob import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# A bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\"\"\"datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\"\"\"\n",
    "#don't worry about data augmentation for now...\n",
    "#data preprocessing is done in another file\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 64, 64, 3) (600, 64, 64, 3)\n",
      "(2000,) (600,)\n",
      "data read and normalization finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "globPaths = ['data/c0small/*.jpg','data/c1small/*.jpg']\n",
    "num_classes = 2\n",
    "numTrain = 1000\n",
    "\n",
    "X_train = np.empty((2000,64,64,3))\n",
    "X_test = np.empty((600,64,64,3))\n",
    "y_train =np.empty((2000))\n",
    "y_test = np.empty((600))\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for folderIndex in range(2):\n",
    "    index = 0\n",
    "    filelist = glob(globPaths[folderIndex])\n",
    "    for filename in filelist:\n",
    "        newImg = misc.imread(filename)\n",
    "        if index<numTrain:\n",
    "            X_train[i] = newImg\n",
    "            y_train[i] = folderIndex\n",
    "            i+=1\n",
    "        else:\n",
    "            X_test[j] = newImg\n",
    "            y_test[j] = folderIndex\n",
    "            j+=1\n",
    "        index+=1\n",
    "        \n",
    "        \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#then data normalization\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "        \n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    \n",
    "print(X_train.shape,X_test.shape)\n",
    "print(y_train.shape,y_test.shape)\n",
    "print(\"data read and normalization finished!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model building and compiling finished!\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 64, 64\n",
    "img_channels = 3\n",
    "#the later are about training...\n",
    "\n",
    "input_shape = (img_rows, img_cols,img_channels)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),padding= 'same',\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=opt, #keras.optimizers.Adadelta(0.01),\n",
    "              metrics=['accuracy'])\n",
    "print(\"model building and compiling finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 600 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 67s - loss: 0.1128 - acc: 0.9615 - val_loss: 0.0322 - val_acc: 0.9917\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 64s - loss: 0.1370 - acc: 0.9615 - val_loss: 0.1479 - val_acc: 0.9483\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 66s - loss: 0.1113 - acc: 0.9650 - val_loss: 0.0933 - val_acc: 0.9617\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 65s - loss: 0.0659 - acc: 0.9795 - val_loss: 0.3999 - val_acc: 0.8633\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 71s - loss: 0.0678 - acc: 0.9785 - val_loss: 0.0087 - val_acc: 0.9967\n",
      "Test loss: 0.00868155636883\n",
      "Test accuracy: 0.996666666667\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 \n",
    "nb_epoch = 5\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "\n",
    "pred = model.predict(X_train,batch_size=10)\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "model.save('keras_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
